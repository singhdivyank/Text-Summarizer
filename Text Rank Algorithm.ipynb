{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Rank algorithm is - **extractive summarization technique**. The assigned problem can be classified as **a single-domain-multiple-document summarization task** which means multiple articles generate a single bullet-point summary on Covid19 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Divyank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import networkx as nx\n",
    "import nltk\n",
    "nltk.download('punkt') # one time execution\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag # for proper nouns\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom functions\n",
    "\n",
    "1. Cue phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cue_phrases():\n",
    "    QPhrases=[\"incidentally\", \"example\", \"anyway\", \"furthermore\",\"according\"\n",
    "            \"first\", \"second\", \"then\", \"now\", \"thus\", \"moreover\", \"therefore\", \"hence\", \"lastly\", \"finally\", \"summary\"]\n",
    "    cue_phrases={}\n",
    "    \n",
    "    for sentence in sent_tokens:\n",
    "        cue_phrases[sentence] = 0\n",
    "        word_tokens = nltk.word_tokenize(sentence)\n",
    "        for word in word_tokens:\n",
    "            if word.lower() in QPhrases:\n",
    "                cue_phrases[sentence] += 1\n",
    "    \n",
    "    maximum_frequency = max(cue_phrases.values())\n",
    "    \n",
    "    for k in cue_phrases.keys():\n",
    "        try:\n",
    "            cue_phrases[k] = cue_phrases[k] / maximum_frequency\n",
    "            cue_phrases[k] = round(cue_phrases[k], 3)\n",
    "        except ZeroDivisionError:\n",
    "            x = 0\n",
    "    \n",
    "    print(cue_phrases.values())\n",
    "    \n",
    "    return cue_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_data():\n",
    "    numeric_data = {}\n",
    "    \n",
    "    for sentence in sent_tokens:\n",
    "        numeric_data[sentence] = 0\n",
    "        word_tokens = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        for k in word_tokens:\n",
    "            if k.isdigit():\n",
    "                numeric_data[sentence] += 1\n",
    "    \n",
    "    maximum_frequency = max(numeric_data.values())\n",
    "    \n",
    "    for k in numeric_data.keys():\n",
    "        try:\n",
    "            numeric_data[k] = (numeric_data[k]/maximum_frequency)\n",
    "            numeric_data[k] = round(numeric_data[k], 3)\n",
    "        except ZeroDivisionError:\n",
    "            x = 0\n",
    "    \n",
    "    print(numeric_data.values())\n",
    "    \n",
    "    return numeric_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_len_score():\n",
    "    sent_len_score={}\n",
    "    \n",
    "    for sentence in sent_tokens:\n",
    "        sent_len_score[sentence] = 0\n",
    "        word_tokens = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        if len(word_tokens) in range(0,10):\n",
    "            sent_len_score[sentence] = 1 - 0.05 * (10 - len(word_tokens))\n",
    "        \n",
    "        elif len(word_tokens) in range(7,20):\n",
    "            sent_len_score[sentence] = 1\n",
    "        \n",
    "        else:\n",
    "            sent_len_score[sentence] = 1 - (0.05) * (len(word_tokens) - 20)\n",
    "    \n",
    "    for k in sent_len_score.keys():\n",
    "        sent_len_score[k] = round(sent_len_score[k], 4)\n",
    "    \n",
    "    print(sent_len_score.values())\n",
    "    \n",
    "    return sent_len_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Sentence position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_position():\n",
    "    sentence_position={}\n",
    "    \n",
    "    d = 1\n",
    "    no_of_sent = len(sent_tokens)\n",
    "    \n",
    "    for i in range(no_of_sent):\n",
    "        a = 1/d\n",
    "        b = 1/(no_of_sent-d+1)\n",
    "        sentence_position[sent_tokens[d-1]] = max(a,b)\n",
    "        d += 1\n",
    "    \n",
    "    for k in sentence_position.keys():\n",
    "        sentence_position[k] = round(sentence_position[k], 3)\n",
    "    \n",
    "    print(sentence_position.values())\n",
    "    \n",
    "    return sentence_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Frequency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency():\n",
    "    freqTable = {}\n",
    "    \n",
    "    for word in word_tokens_refined:    \n",
    "        if word in freqTable:         \n",
    "            freqTable[word] += 1    \n",
    "        else:         \n",
    "            freqTable[word] = 1\n",
    "    \n",
    "    for k in freqTable.keys():\n",
    "        freqTable[k] = math.log10(1 + freqTable[k])\n",
    "\n",
    "    #Compute word frequnecy score of each sentence\n",
    "    word_frequency = {}\n",
    "    \n",
    "    for sentence in sent_tokens:\n",
    "        word_frequency[sentence] = 0\n",
    "        e = nltk.word_tokenize(sentence)\n",
    "        f = []\n",
    "        \n",
    "        for word in e:\n",
    "            f.append(PorterStemmer().stem(word))\n",
    "        \n",
    "        for word,freq in freqTable.items():\n",
    "            if word in f:\n",
    "                word_frequency[sentence] += freq\n",
    "    \n",
    "    maximum = max(word_frequency.values())\n",
    "    \n",
    "    for key in word_frequency.keys():\n",
    "        try:\n",
    "            word_frequency[key] = word_frequency[key]/maximum\n",
    "            word_frequency[key] = round(word_frequency[key],3)\n",
    "        except ZeroDivisionError:\n",
    "            x = 0\n",
    "    \n",
    "    print(word_frequency.values())\n",
    "    \n",
    "    return word_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Upper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_case():\n",
    "    upper_case={}\n",
    "    \n",
    "    for sentence in sent_tokens:\n",
    "        upper_case[sentence] = 0\n",
    "        word_tokens = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        for k in word_tokens:\n",
    "            if k.isupper():\n",
    "                upper_case[sentence] += 1\n",
    "    \n",
    "    maximum_frequency = max(upper_case.values())\n",
    "    \n",
    "    for k in upper_case.keys():\n",
    "        try:\n",
    "            upper_case[k] = (upper_case[k]/maximum_frequency)\n",
    "            upper_case[k] = round(upper_case[k], 3)\n",
    "        except ZeroDivisionError:\n",
    "            x = 0\n",
    "    \n",
    "    print(upper_case.values())\n",
    "    \n",
    "    return upper_case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proper_noun():\n",
    "    proper_noun={}\n",
    "    \n",
    "    for sentence in sent_tokens:\n",
    "        tagged_sent = pos_tag(sentence.split())\n",
    "        propernouns = [word for word, pos in tagged_sent if pos == 'NNP']\n",
    "        proper_noun[sentence]=len(propernouns)\n",
    "    \n",
    "    maximum_frequency = max(proper_noun.values())\n",
    "    \n",
    "    for k in proper_noun.keys():\n",
    "        try:\n",
    "            proper_noun[k] = (proper_noun[k]/maximum_frequency)\n",
    "            proper_noun[k] = round(proper_noun[k], 3)\n",
    "        except ZeroDivisionError:\n",
    "            x = 0\n",
    "    print(proper_noun.values())\n",
    "    \n",
    "    return proper_noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Word matches with heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_match():\n",
    "    head_match={}\n",
    "    heading=sent_tokens[0]\n",
    "    \n",
    "    for sentence in sent_tokens:\n",
    "        head_match[sentence]=0\n",
    "        word_tokens = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        for k in word_tokens:\n",
    "            if k not in my_stopwords:\n",
    "                k = PorterStemmer().stem(k)\n",
    "                \n",
    "                if k in PorterStemmer().stem(heading):\n",
    "                    head_match[sentence] += 1\n",
    "    \n",
    "    maximum_frequency = max(head_match.values())\n",
    "    \n",
    "    for k in head_match.keys():\n",
    "        try:\n",
    "            head_match[k] = (head_match[k]/maximum_frequency)\n",
    "            head_match[k] = round(head_match[k], 3)\n",
    "        except ZeroDivisionError:\n",
    "            x = 0\n",
    "    \n",
    "    print(head_match.values())\n",
    "    \n",
    "    return head_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [a, b, c, d, upper, f, g, h, key, label]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['a','b','c', 'd', 'upper', 'f','g','h','key','label'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
      "dict_values([0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
      "dict_values([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0])\n",
      "dict_values([0.8, -2.25, 1, -0.1, 1.0, 1, 0.6, 0.35, 1, 0.85])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.2, 0.25, 0.333, 0.5, 1.0])\n",
      "dict_values([1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0])\n",
      "dict_values([0.87, 1.0, 0.304, 0.478, 0.13, 0.174, 0.13, 0.13, 0.261, 0.217])\n",
      "dict_values([0.484, 1.0, 0.189, 0.755, 0.238, 0.158, 0.305, 0.191, 0.212, 0.263])\n",
      "dict_values([0.364, 1.0, 0.0, 0.909, 0.091, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
      "dict_values([0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0])\n",
      "dict_values([0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0])\n",
      "dict_values([0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0])\n",
      "dict_values([1, 0.6, 1, 0.8, 0.8, -0.05, 1, 1, 1.0, 0.45, -0.35, -0.8, 1, 0.4, 0.15, 0.5])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.167, 0.143, 0.125, 0.125, 0.143, 0.167, 0.2, 0.25, 0.333, 0.5, 1.0])\n",
      "dict_values([0.333, 0.667, 0.333, 0.667, 0.0, 0.0, 0.667, 0.0, 0.333, 0.333, 0.0, 0.333, 0.0, 0.0, 1.0, 0.333])\n",
      "dict_values([1.0, 0.455, 0.182, 0.182, 0.182, 0.273, 0.182, 0.182, 0.273, 0.091, 0.091, 0.364, 0.091, 0.182, 0.364, 0.364])\n",
      "dict_values([0.497, 0.904, 0.479, 0.545, 0.471, 0.545, 0.504, 0.516, 0.549, 0.569, 0.762, 0.894, 0.427, 0.667, 1.0, 0.53])\n",
      "dict_values([0.5, 0.625, 0.0, 0.125, 0.0, 0.0, 0.625, 0.125, 0.125, 0.125, 0.875, 1.0, 0.0, 0.25, 0.875, 0.125])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
      "dict_values([1, 0.6, -0.65, 0.5, 0.35, 0.9, 0.15, 0.5, 0.1, 0.9, 0.35, 0.95, -0.25, 0.95, 0.9, 1.0])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.167, 0.143, 0.125, 0.125, 0.143, 0.167, 0.2, 0.25, 0.333, 0.5, 1.0])\n",
      "dict_values([0.25, 0.0, 0.75, 0.75, 0.5, 0.25, 1.0, 0.75, 0.0, 0.0, 0.5, 0.5, 1.0, 0.25, 0.5, 0.25])\n",
      "dict_values([1.0, 0.286, 0.429, 0.714, 0.714, 0.571, 0.857, 0.143, 0.286, 0.143, 0.143, 0.286, 0.857, 0.286, 0.429, 0.429])\n",
      "dict_values([0.427, 0.688, 1.0, 0.674, 0.815, 0.532, 0.833, 0.555, 0.588, 0.342, 0.761, 0.383, 0.982, 0.356, 0.474, 0.642])\n",
      "dict_values([0.2, 0.3, 0.4, 0.9, 0.7, 0.1, 0.8, 0.6, 0.0, 0.0, 0.6, 0.3, 1.0, 0.0, 0.1, 0.2])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5])\n",
      "dict_values([1, 0.15, 1, 0.7, -0.5, 0.55, 1, 0.75, -1.4, 1, 0.85, 1, 1.0, 0.5, -0.45, 1, 0.55])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.167, 0.143, 0.125, 0.111, 0.1, 0.111, 0.125, 0.143, 0.167, 0.5, 1.0, 0.333])\n",
      "dict_values([0.333, 0.333, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.333, 0.0, 1.0, 0.0, 0.0])\n",
      "dict_values([1.0, 0.375, 0.125, 0.125, 0.5, 0.5, 0.125, 0.125, 0.625, 0.125, 0.375, 0.25, 0.25, 0.375, 0.625, 0.125, 0.375])\n",
      "dict_values([0.432, 0.648, 0.213, 0.581, 0.786, 0.633, 0.276, 0.466, 1.0, 0.213, 0.662, 0.405, 0.384, 0.689, 0.823, 0.346, 0.689])\n",
      "dict_values([0.214, 0.214, 0.143, 0.143, 0.286, 0.357, 0.143, 0.0, 0.857, 0.0, 0.214, 0.0, 0.143, 0.214, 1.0, 0.0, 0.214])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([1, 0.7, 0.45, 1.0, 1, 0.55, 0.85, -0.85, 0.45, 0.3, 1.0, 0.9, 0.9, 0.3])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.167, 0.143, 0.143, 0.167, 0.2, 0.25, 0.333, 0.5, 1.0])\n",
      "dict_values([0.0, 1.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 1.0, 0.0, 0.0, 0.5])\n",
      "dict_values([1.0, 0.273, 0.273, 0.273, 0.091, 0.273, 0.273, 0.545, 0.182, 0.182, 0.364, 0.091, 0.091, 0.273])\n",
      "dict_values([0.448, 0.77, 0.748, 0.421, 0.476, 0.515, 0.596, 1.0, 0.453, 0.558, 0.457, 0.209, 0.411, 0.644])\n",
      "dict_values([0.6, 1.0, 0.4, 0.2, 0.2, 0.6, 0.0, 0.4, 0.0, 0.2, 0.8, 0.0, 0.0, 0.2])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([1, 0.6, 0.0, 1, 0.2, 0.85, -0.2, 0.45, 0.95, 0.9, 1, 1, 0.75, 0.9, 1.0, 1, 0.15, 0.45, 1])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.167, 0.143, 0.125, 0.111, 0.1, 0.111, 0.125, 0.143, 0.167, 0.2, 0.25, 0.333, 0.5, 1.0])\n",
      "dict_values([0.333, 1.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.333, 0.0, 0.0])\n",
      "dict_values([1.0, 0.182, 0.364, 0.182, 0.182, 0.273, 0.455, 0.273, 0.273, 0.182, 0.091, 0.091, 0.091, 0.091, 0.273, 0.091, 0.182, 0.273, 0.091])\n",
      "dict_values([0.608, 0.414, 0.928, 0.567, 0.876, 0.621, 1.0, 0.747, 0.457, 0.882, 0.519, 0.327, 0.213, 0.36, 0.482, 0.371, 0.826, 0.462, 0.389])\n",
      "dict_values([0.5, 0.167, 0.0, 0.0, 0.833, 0.0, 0.0, 0.0, 0.0, 0.333, 0.167, 0.0, 0.333, 0.0, 0.0, 0.167, 1.0, 0.0, 0.167])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0])\n",
      "dict_values([1, 0.55, 0.85, 1, 1, 0.9, 1, 1, 1, 1, 1, 0.25])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.167, 0.167, 0.2, 0.25, 0.333, 0.5, 1.0])\n",
      "dict_values([0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0])\n",
      "dict_values([1.0, 0.5, 0.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.2, 0.2, 0.6])\n",
      "dict_values([0.603, 0.691, 0.555, 0.314, 0.381, 0.802, 0.381, 0.286, 0.531, 0.368, 0.273, 1.0])\n",
      "dict_values([0.5, 0.167, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.333, 0.0, 0.167, 0.833])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0])\n",
      "dict_values([1, 0.8, 0.85, 1, 1, 1, 0.65, 0.95, 0.85, 1, 0.6, 0.2, 0.95, 1, 0.65, 0.4, 0.35, 1.0, 1, 0.7, 1, 1])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.167, 0.143, 0.125, 0.111, 0.1, 0.091, 0.091, 0.1, 0.111, 0.125, 0.143, 0.167, 0.2, 0.25, 0.333, 0.5, 1.0])\n",
      "dict_values([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0])\n",
      "dict_values([1.0, 0.625, 0.375, 0.25, 0.125, 0.375, 0.25, 0.25, 0.25, 0.375, 0.125, 0.25, 0.125, 0.25, 0.125, 0.25, 0.5, 0.125, 0.125, 0.25, 0.125, 0.125])\n",
      "dict_values([0.544, 0.917, 0.774, 0.483, 0.285, 0.604, 0.786, 0.534, 0.55, 0.576, 0.713, 0.872, 0.521, 0.526, 0.424, 0.918, 1.0, 0.671, 0.56, 0.757, 0.743, 0.523])\n",
      "dict_values([0.333, 0.167, 0.333, 0.333, 0.0, 0.167, 0.333, 0.333, 0.5, 0.333, 1.0, 0.167, 0.0, 0.333, 0.667, 0.167, 0.333, 0.167, 0.333, 0.167, 1.0, 0.167])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "dict_values([0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0])\n",
      "dict_values([1, 0.45, 0.45, 0.3, 1, 1, 0.45, 0.35, 0.75, 1, 0.65, 0.85, 0.5, 0.35, 0.85, 0.3, 0.9])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.167, 0.143, 0.125, 0.111, 0.125, 0.143, 0.167, 0.2, 0.25, 0.333, 0.5, 1.0])\n",
      "dict_values([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0])\n",
      "dict_values([1.0, 0.286, 0.429, 0.429, 0.286, 0.286, 0.571, 0.429, 0.143, 0.143, 0.571, 0.286, 0.857, 0.714, 0.143, 0.286, 0.143])\n",
      "dict_values([0.497, 0.691, 0.479, 1.0, 0.557, 0.757, 0.949, 0.722, 0.222, 0.186, 0.984, 0.587, 0.772, 0.799, 0.422, 0.715, 0.68])\n",
      "dict_values([0.182, 0.909, 0.091, 0.364, 0.182, 0.364, 0.273, 0.0, 0.182, 0.0, 0.364, 0.091, 1.0, 0.091, 0.273, 0.0, 0.182])\n",
      "dict_values([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0])\n",
      "dict_values([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0])\n",
      "dict_values([0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.0, 0.333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.667, 0.0, 0.0, 0.0, 0.333])\n",
      "dict_values([1, 1, 0.9, 1, 1, 0.65, 0.25, 1.0, 0.7, 0.35, -0.4, 0.95, 0.65, 0.7, 0.95, 0.5, 0.9, 0.65, 0.9, 0.55, -0.15, -1.4, 0.7, 1, -0.15, 0.2, -1.1, 0.25, 0.45, 0.1, 0.9, 0.8, 0.7, 0.75, 0.3, 1, 1, 0.75, 0.6, 1, -0.8, 1.0, -0.05])\n",
      "dict_values([1.0, 0.5, 0.333, 0.25, 0.2, 0.167, 0.143, 0.125, 0.111, 0.1, 0.091, 0.083, 0.077, 0.071, 0.067, 0.062, 0.059, 0.056, 0.053, 0.05, 0.048, 0.045, 0.048, 0.05, 0.053, 0.056, 0.059, 0.062, 0.067, 0.071, 0.077, 0.083, 0.091, 0.1, 0.111, 0.125, 0.143, 0.167, 0.2, 0.25, 0.333, 0.5, 1.0])\n",
      "dict_values([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.667, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.333, 0.0, 0.0, 0.0, 0.0, 0.333, 0.667])\n",
      "dict_values([1.0, 0.125, 0.125, 0.125, 0.25, 0.0, 0.125, 0.25, 0.125, 0.125, 0.0, 0.25, 0.125, 0.25, 0.125, 0.25, 0.0, 0.125, 0.25, 0.125, 0.0, 0.375, 0.125, 0.0, 0.375, 0.25, 0.625, 0.25, 0.125, 0.375, 0.0, 0.0, 0.0, 0.125, 0.25, 0.0, 0.125, 0.125, 0.0, 0.0, 0.25, 0.375, 0.375])\n",
      "dict_values([0.139, 0.268, 0.299, 0.132, 0.319, 0.342, 0.37, 0.304, 0.371, 0.515, 0.58, 0.107, 0.492, 0.528, 0.192, 0.28, 0.399, 0.337, 0.329, 0.358, 0.7, 0.817, 0.402, 0.284, 0.645, 0.625, 1.0, 0.482, 0.65, 0.675, 0.369, 0.185, 0.713, 0.48, 0.454, 0.325, 0.257, 0.372, 0.675, 0.419, 0.663, 0.28, 0.592])\n",
      "dict_values([0.154, 0.077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.385, 0.0, 0.538, 0.0, 0.0, 0.154, 0.0, 0.154, 0.231, 0.462, 0.0, 0.385, 0.385, 0.231, 0.0, 0.0, 0.692, 0.077, 1.0, 0.0, 0.308, 0.0, 0.077, 0.0, 0.154, 0.154, 0.077, 0.0, 0.077, 0.0, 0.0, 0.154, 0.077, 0.077, 0.538])\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 5740: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5586f972b0aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfilelist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0msent_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 5740: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "path = 'D:/COVID_19_dataset/documents/'\n",
    "\n",
    "filelist = os.listdir(path)\n",
    "\n",
    "for file in filelist:\n",
    "    f = open(path + file, \"r\")\n",
    "    text = f.read()\n",
    "    \n",
    "    sent_tokens = nltk.sent_tokenize(text)\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    word_tokens_lower = [word.lower() for word in word_tokens]\n",
    "    \n",
    "    my_stopwords = list(set(stopwords.words('english')))\n",
    "    \n",
    "    word_tokens_refined = [x for x in word_tokens_lower if x not in my_stopwords]\n",
    "    \n",
    "    Cue_phrases = list(cue_phrases().values())\n",
    "    Key = list(cue_phrases().keys())\n",
    "    Numeric_data = list(numeric_data().values())\n",
    "    Sent_length_score = list(sent_len_score().values())\n",
    "    Sentence_position = list(sentence_position().values())\n",
    "    Upper_case = list(upper_case().values())\n",
    "    Header_match = list(head_match().values())\n",
    "    Word_frequency = list(word_frequency().values())\n",
    "    Proper_noun = list(proper_noun().values())\n",
    "    \n",
    "    label = {}\n",
    "    for sentence in sent_tokens:\n",
    "        label[sentence] = 0\n",
    "                \n",
    "    o = list(label.values())\n",
    "    df = df.append(pd.DataFrame({'a': Cue_phrases,'b': Numeric_data,'c': Sent_length_score,'d': Sentence_position,\n",
    "                                 'upper': Upper_case, 'f': Header_match, 'g': Word_frequency,'h': Proper_noun,\n",
    "                                 'key': Key,'label': o}), ignore_index = True)\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.to_csv('D:/COVID_19_dataset/documents/output.csv', index = False)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>upper</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>key</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.364</td>\n",
       "      <td>Success from two leading coronavirus vaccine p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>The fact that two coronavirus vaccines recentl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.000</td>\n",
       "      <td>The studies showed both vaccines provided stro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.909</td>\n",
       "      <td>\"With the very good news from Pfizer and Moder...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.091</td>\n",
       "      <td>While Gates didn't delve into the scientific r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     a    b     c      d  upper      f      g      h  \\\n",
       "0  0.0  0.0  0.80  1.000    1.0  0.870  0.484  0.364   \n",
       "1  0.0  0.0 -2.25  0.500    1.0  1.000  1.000  1.000   \n",
       "2  0.0  0.0  1.00  0.333    0.0  0.304  0.189  0.000   \n",
       "3  1.0  0.0 -0.10  0.250    0.0  0.478  0.755  0.909   \n",
       "4  0.0  0.0  1.00  0.200    0.0  0.130  0.238  0.091   \n",
       "\n",
       "                                                 key  label  \n",
       "0  Success from two leading coronavirus vaccine p...      0  \n",
       "1  The fact that two coronavirus vaccines recentl...      0  \n",
       "2  The studies showed both vaccines provided stro...      0  \n",
       "3  \"With the very good news from Pfizer and Moder...      0  \n",
       "4  While Gates didn't delve into the scientific r...      0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('D:/COVID_19_dataset/documents/output.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>upper</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>key</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.000</td>\n",
       "      <td>\"We talk about the 90/10 divide in global heal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.154</td>\n",
       "      <td>This is part of that story,\" Ms Wenham said.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.077</td>\n",
       "      <td>\"But there's a difference between the fact tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.077</td>\n",
       "      <td>A landmark global vaccine plan known as Covax ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.538</td>\n",
       "      <td>The joint initiative - between the Gavi vaccin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       a      b     c      d  upper      f      g      h  \\\n",
       "181  0.0  0.667  0.60  0.200  0.000  0.000  0.675  0.000   \n",
       "182  0.0  0.000  1.00  0.250  0.000  0.000  0.419  0.154   \n",
       "183  1.0  0.000 -0.80  0.333  0.000  0.250  0.663  0.077   \n",
       "184  0.0  0.000  1.00  0.500  0.333  0.375  0.280  0.077   \n",
       "185  0.0  0.333 -0.05  1.000  0.667  0.375  0.592  0.538   \n",
       "\n",
       "                                                   key  label  \n",
       "181  \"We talk about the 90/10 divide in global heal...      0  \n",
       "182       This is part of that story,\" Ms Wenham said.      0  \n",
       "183  \"But there's a difference between the fact tha...      0  \n",
       "184  A landmark global vaccine plan known as Covax ...      0  \n",
       "185  The joint initiative - between the Gavi vaccin...      0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Here there are 10 columns the key column is useful because it has text entries of all the files. Printing some values to see what they look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Success from two leading coronavirus vaccine programs likely means other frontrunners will also show strong protection against COVID-19, Bill Gates said Tuesday.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['key'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The development comes nearly 10 months after news of the coronavirus began to emerge from Wuhan, China.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['key'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    186\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "**Objective**: To generate a single summary for all articles\n",
    "\n",
    "## Step 1: Split text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Success from two leading coronavirus vaccine programs likely means other frontrunners will also show strong protection against COVID-19, Bill Gates said Tuesday.',\n",
       " 'The fact that two coronavirus vaccines recently showed strong protection against COVID-19 bodes well for other leading programs led by AstraZeneca, Novavax, and Johnson & Johnson, Bill Gates said Tuesday.The billionaire Microsoft founder and philanthropist said it will be easier to boost manufacturing and distribute these other shots to the entire world, particularly developing nations.The vaccine space has seen a flurry of good news in recent days, marked by overwhelming success in late-stage trials by both Pfizer and Moderna.',\n",
       " 'The studies showed both vaccines provided strong protection against the virus compared to a placebo.',\n",
       " '\"With the very good news from Pfizer and Moderna, we think it\\'s now likely that AstraZeneca, Novavax, and Johnson & Johnson will also likely show very strong efficacy,\" Gates told journalist Andrew Ross Sorkin.',\n",
       " \"While Gates didn't delve into the scientific rationale behind that prediction, many scientists hold the same hope.\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [] # empty list\n",
    "\n",
    "for s in data['key']:\n",
    "    sentences.append(sent_tokenize(s))\n",
    "    \n",
    "sentences = [y for x in sentences for y in x] # flatten list\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "\n",
    "f = open('E:/Jupyterfiles/ML_practice/Stanford/glove.6B/glove.6B.100d.txt', encoding = 'utf-8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word vectors for 400K different terms are stored in the dictionary\n",
    "\n",
    "## Step 3: Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = data[\"key\"] != \"\"\n",
    "data_clean = data[filter]\n",
    "data_clean = data_clean.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    \n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Success from two leading coronavirus vaccine programs likely means other frontrunners will also show strong protection against COVID Bill Gates said Tuesday ',\n",
       " 'The fact that two coronavirus vaccines recently showed strong protection against COVID bodes well for other leading programs led by AstraZeneca Novavax and Johnson Johnson Bill Gates said Tuesday The billionaire Microsoft founder and philanthropist said it will be easier to boost manufacturing and distribute these other shots to the entire world particularly developing nations The vaccine space has seen flurry of good news in recent days marked by overwhelming success in late stage trials by both Pfizer and Moderna ',\n",
       " 'The studies showed both vaccines provided strong protection against the virus compared to placebo ',\n",
       " ' With the very good news from Pfizer and Moderna we think it now likely that AstraZeneca Novavax and Johnson Johnson will also likely show very strong efficacy Gates told journalist Andrew Ross Sorkin ',\n",
       " 'While Gates didn delve into the scientific rationale behind that prediction many scientists hold the same hope ']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "sentences = list(data[\"key\"])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))\n",
    "\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in my_stopwords])\n",
    "    \n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['success two leading coronavirus vaccine programs likely means frontrunners also show strong protection covid bill gates said tuesday',\n",
       " 'the fact two coronavirus vaccines recently showed strong protection covid bodes well leading programs led astrazeneca novavax johnson johnson bill gates said tuesday the billionaire microsoft founder philanthropist said easier boost manufacturing distribute shots entire world particularly developing nations the vaccine space seen flurry good news recent days marked overwhelming success late stage trials pfizer moderna',\n",
       " 'the studies showed vaccines provided strong protection virus compared placebo',\n",
       " 'with good news pfizer moderna think likely astrazeneca novavax johnson johnson also likely show strong efficacy gates told journalist andrew ross sorkin',\n",
       " 'while gates delve scientific rationale behind prediction many scientists hold hope']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sentences = [remove_stopwords(r.split()) for r in X]\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "\n",
    "clean_sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 'clean_sentences' to create vectors for sentences in data with the help of GloVe word vectors\n",
    "\n",
    "## Step 4: Vector representation of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "\n",
    "f = open('E:/Jupyterfiles/ML_practice/Stanford/glove.6B/glove.6B.100d.txt', encoding = 'utf-8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1: ], dtype = 'float32')\n",
    "    word_embeddings[word] = coefs\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split()) + 0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    \n",
    "    sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Similarity between sentences\n",
    "\n",
    "**Algorithm used**: Cosine similarity\n",
    "\n",
    "**Approach**: Create an empty similarity matrix and populate it with cosine similarities of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = np.zeros([len(sentences), len(sentences)]) # zero matrix of size nXn\n",
    "\n",
    "# initialise matrix with cosine similarity scores\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i!=j:\n",
    "            sim_matrix[i][j] = cosine_similarity(sentence_vectors[i].reshape(1, 100), sentence_vectors[j].reshape(1, 100))[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Apply PageRank algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Summary extraction\n",
    "\n",
    "Extract top-N sentences based on rankings for summary generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meanwhile, efforts to develop an effective vaccine are continuing - although the World Health Organization (WHO) has warned that the death toll could hit two million before one is widely available.\n",
      "As ministers struggle to get test-and-trace on track, BBC News spoke to key government figures, scientists and health officials who were involved from the very start to establish what went wrong - and, crucially, whether the system can be fixed to hold the virus in check until vaccines come to the rescue.\n",
      "From Monday, under new government restrictions designed to tackle the fresh outbreak, residents will only be allowed to see one other person from outside their household and should work from home if possible.\n",
      "Delivering a limited supply to the world\n",
      "Andrea Taylor, who has been leading the Duke analysis, said the combination of advance purchase agreements and limits on the number of doses that can be manufactured in the next couple of years meant \"we're heading into a scenario where the rich countries will have vaccines and the poorer countries are unlikely to have access\".\n",
      "The paper says ministers have approved a plan to cut the isolation time to just five days, after which travellers would face tests for the virus that return results within an hour.\n",
      "Moderna Covid vaccine shows nearly 95% protection\n",
      "UK orders 5m doses of Moderna vaccine\n",
      "How a 'warm vaccine' could help India tackle Covid\n",
      "Rachel Silverman, a policy analyst at the Center for Global Development think-tank in the US, said the most promising vaccines \"are largely covered by advanced purchase agreements, mostly from wealthy countries\".\n",
      "“So New Zealand took a precautionary approach and on 26 March, apart from essential workers, the entire country was required to self-quarantine at home.” WHO Regional Director for the Western Pacific, Dr Takeshi Kasai, explains that New Zealand combined strict physical distancing with strong testing, contact tracing, clinical management of those infected, and clear and regular public communication.\n",
      "The fact that two coronavirus vaccines recently showed strong protection against COVID-19 bodes well for other leading programs led by AstraZeneca, Novavax, and Johnson & Johnson, Bill Gates said Tuesday.The billionaire Microsoft founder and philanthropist said it will be easier to boost manufacturing and distribute these other shots to the entire world, particularly developing nations.The vaccine space has seen a flurry of good news in recent days, marked by overwhelming success in late-stage trials by both Pfizer and Moderna.\n",
      "The investigation found a system performing worst in the areas where it is needed the most and still struggling with the legacy of decisions that were made at the outset.\n",
      "But she added: \"There is very little likelihood that it will make it to low- and middle-income countries by the end of next year, at least in any significant numbers for mass vaccination.\"\n"
     ]
    }
   ],
   "source": [
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse = True)\n",
    "\n",
    "for i in range(10):\n",
    "    print(ranked_sentences[i][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
